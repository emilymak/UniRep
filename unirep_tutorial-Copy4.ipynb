{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_FULL_1900_DIM_MODEL = False # if True use 1900 dimensional model, else use 64 dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if USE_FULL_1900_DIM_MODEL:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/1900_weights/ 1900_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler1900 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./1900_weights\"\n",
    "    \n",
    "else:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/64_weights/ 64_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler64 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./64_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "b = babbler(batch_size=batch_size, model_path=MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before you can train your model, \n",
    "sequences = []\n",
    "with open(\"emi_neg_seqs_5A_4.txt\", \"r\") as source:\n",
    "    with open(\"formatted.txt\", \"w\") as destination:\n",
    "        for i,seq in enumerate(source):\n",
    "            seq = seq.strip()\n",
    "            sequences.append(seq)\n",
    "            if b.is_valid_seq(seq) and len(seq) < 275: \n",
    "                formatted = \",\".join(map(str,b.format_seq(seq)))\n",
    "                destination.write(formatted)\n",
    "                destination.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n",
      "rep\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-abae7d084eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mavg_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rep_hs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0maverage_hidden_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfinal_hidden_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## \n",
    "average_hidden_list = []\n",
    "final_hidden_list = []\n",
    "hs_list = []\n",
    "final_cell_list = []\n",
    "\n",
    "\n",
    "num2 = range(0, 50)\n",
    "x = 0\n",
    "y = 50\n",
    "for i in num2:\n",
    "    num1 = range(x, y)\n",
    "    for j in num1:\n",
    "        avg_hidden, final_hidden, final_cell, hs_out = (b.get_rep_hs(sequences[j]))\n",
    "        average_hidden_list.append(avg_hidden)\n",
    "        final_hidden_list.append(final_hidden)\n",
    "        final_cell_list.append(final_cell)\n",
    "        hs_list.append(hs_out)\n",
    "        print('rep')\n",
    "    x = x + 50\n",
    "    y = y + 50\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.165004  0.367741  0.358379 -0.652705  0.310745  0.112354  0.319343   \n",
      "1     -0.156224  0.020769 -0.005827 -0.946178 -0.175482  0.024309  0.109565   \n",
      "2     -0.076103  0.039707 -0.068597 -0.982010 -0.074966 -0.034950  0.068059   \n",
      "3     -0.058311  0.053986 -0.086645 -0.987298 -0.051170 -0.053258  0.027245   \n",
      "4     -0.003677  0.076352 -0.109282 -0.989626 -0.047260 -0.053738  0.027420   \n",
      "5     -0.038077  0.097799 -0.118403 -0.985669 -0.050498 -0.049629  0.037225   \n",
      "6      0.025612  0.064812 -0.113066 -0.984256 -0.059101 -0.051365  0.012105   \n",
      "7      0.049025  0.082313 -0.087995 -0.990922 -0.041432 -0.112213  0.029521   \n",
      "8      0.006583  0.135147 -0.078896 -0.990121 -0.015207 -0.122814  0.061241   \n",
      "9      0.008933  0.127565 -0.079991 -0.987132 -0.015113 -0.152265  0.025057   \n",
      "10     0.077980  0.103989 -0.058407 -0.984953 -0.021431 -0.128419  0.045190   \n",
      "11     0.026909  0.202782 -0.059125 -0.989396 -0.018550 -0.105592  0.042051   \n",
      "12     0.035936  0.159157 -0.088357 -0.969360 -0.026968 -0.056122  0.024937   \n",
      "13     0.059801  0.093132 -0.055557 -0.975596  0.002670 -0.008614  0.013640   \n",
      "14     0.062550  0.186707 -0.051769 -0.991475 -0.006423 -0.028718 -0.005721   \n",
      "15    -0.011214  0.167544 -0.043082 -0.991248 -0.011145 -0.035595  0.034426   \n",
      "16     0.058632  0.181080 -0.040065 -0.987976  0.017790 -0.069905  0.001385   \n",
      "17     0.029970  0.136943 -0.033349 -0.990374  0.005956 -0.082746  0.020479   \n",
      "18     0.000441  0.139913 -0.038506 -0.990033 -0.004634 -0.071803  0.016742   \n",
      "19     0.010915  0.160127 -0.046023 -0.979786 -0.002303 -0.025715 -0.015243   \n",
      "20     0.000773  0.222996 -0.049428 -0.987504  0.004572 -0.033628  0.006430   \n",
      "21     0.025222  0.186978 -0.068745 -0.972566  0.009812 -0.064642  0.029696   \n",
      "22     0.014874  0.136089 -0.071880 -0.985632 -0.014120 -0.041224 -0.008277   \n",
      "23     0.013732  0.130824 -0.094137 -0.940037  0.016129 -0.021758  0.006848   \n",
      "24     0.040988  0.195814 -0.055992 -0.972032  0.064121 -0.009580  0.047241   \n",
      "25     0.027895  0.113103 -0.053742 -0.974870  0.035050 -0.019315 -0.016683   \n",
      "26     0.006742  0.205769 -0.055555 -0.977195  0.057405 -0.013741  0.014226   \n",
      "27     0.018036  0.158473 -0.039804 -0.984140  0.025614 -0.047718 -0.009029   \n",
      "28     0.056865  0.188677 -0.014084 -0.986501  0.030536 -0.090299  0.052287   \n",
      "29    -0.006308  0.198464 -0.095512 -0.978152  0.017377 -0.077658  0.031064   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "57970  0.039076  0.145973 -0.180621 -0.944543 -0.053564 -0.283815  0.036587   \n",
      "57971  0.047078  0.138747 -0.222025 -0.886100 -0.056785 -0.227853  0.032999   \n",
      "57972  0.027926  0.142826 -0.170356 -0.934530 -0.031202 -0.225002  0.066594   \n",
      "57973  0.067052  0.182060 -0.163893 -0.946311 -0.028695 -0.357287  0.159578   \n",
      "57974  0.073938  0.171589 -0.111634 -0.979308 -0.044234 -0.413071  0.132352   \n",
      "57975  0.088360  0.222781 -0.094157 -0.985474 -0.022892 -0.478291  0.100447   \n",
      "57976  0.032610  0.205106 -0.078107 -0.985826 -0.005994 -0.470329  0.073912   \n",
      "57977  0.071286  0.266609 -0.047584 -0.988227 -0.005113 -0.466345  0.061545   \n",
      "57978  0.044965  0.355929 -0.076288 -0.984347 -0.022062 -0.468930  0.073270   \n",
      "57979  0.052867  0.332716 -0.075993 -0.975273 -0.023027 -0.477135  0.069560   \n",
      "57980  0.024109  0.287562 -0.192347 -0.959650 -0.025606 -0.477067  0.057585   \n",
      "57981 -0.012272  0.230562 -0.182435 -0.951888 -0.003914 -0.421272  0.075238   \n",
      "57982 -0.014432  0.150129 -0.172088 -0.892971  0.003319 -0.387477  0.080585   \n",
      "57983 -0.017607  0.173035 -0.175767 -0.915247  0.002558 -0.358202  0.079751   \n",
      "57984  0.065617  0.171887 -0.216213 -0.901305 -0.010620 -0.405960  0.163597   \n",
      "57985  0.049062  0.157892 -0.164573 -0.949101 -0.012921 -0.466590  0.054340   \n",
      "57986  0.062886  0.129825 -0.159258 -0.959919 -0.025172 -0.439749  0.024228   \n",
      "57987  0.074782  0.188840 -0.138374 -0.974414 -0.034476 -0.438319  0.109412   \n",
      "57988  0.042036  0.166282 -0.169392 -0.953716 -0.004024 -0.392030  0.086788   \n",
      "57989  0.035696  0.174819 -0.153578 -0.902281 -0.005186 -0.400643  0.075623   \n",
      "57990  0.020472  0.150796 -0.122955 -0.940604 -0.004377 -0.417294  0.078581   \n",
      "57991  0.073260  0.111680 -0.140220 -0.931579 -0.014577 -0.410518  0.065448   \n",
      "57992  0.040397  0.192310 -0.111630 -0.953372 -0.004929 -0.464764  0.092682   \n",
      "57993  0.122268  0.229463 -0.118287 -0.979192 -0.012579 -0.539192  0.091853   \n",
      "57994  0.100243  0.260712 -0.106273 -0.987276 -0.023563 -0.554705  0.115321   \n",
      "57995  0.061852  0.264542 -0.160409 -0.982132 -0.027540 -0.490376  0.073575   \n",
      "57996  0.124249  0.322073 -0.059424 -0.975347 -0.020374 -0.502126  0.108794   \n",
      "57997  0.004153  0.231326 -0.290492 -0.926380 -0.039754 -0.368188  0.079029   \n",
      "57998  0.086726  0.214718 -0.300939 -0.788119 -0.010701 -0.386049  0.104445   \n",
      "57999  0.081485  0.137243 -0.226561 -0.743894 -0.019721 -0.423274  0.096656   \n",
      "\n",
      "             7         8         9     ...           54        55        56  \\\n",
      "0      0.479474  0.566543  0.441343    ...     0.537194  0.512816  0.240133   \n",
      "1      0.108014  0.036832 -0.132778    ...     0.043055  0.063621 -0.021644   \n",
      "2      0.092981  0.163093 -0.054431    ...     0.037193  0.030412  0.071537   \n",
      "3      0.115106  0.120097 -0.164142    ...     0.046072  0.025641  0.055651   \n",
      "4      0.155599  0.163205 -0.143081    ...     0.053135  0.030944  0.110881   \n",
      "5      0.159635  0.174306 -0.124355    ...     0.062251  0.020353  0.117015   \n",
      "6      0.186465  0.157787 -0.161483    ...     0.067689  0.023296  0.123832   \n",
      "7      0.176079  0.200251 -0.158324    ...     0.054021  0.026638  0.132952   \n",
      "8      0.169030  0.212899 -0.221204    ...     0.060218  0.005316  0.015762   \n",
      "9      0.210119  0.217192 -0.265375    ...     0.067813  0.014017  0.088167   \n",
      "10     0.250093  0.124040 -0.227202    ...     0.056659 -0.003319  0.066461   \n",
      "11     0.147191  0.169260 -0.306931    ...     0.050413 -0.009246  0.021361   \n",
      "12     0.315876  0.157280 -0.265005    ...     0.095552 -0.015065  0.081112   \n",
      "13     0.342136  0.193036 -0.279929    ...     0.112255 -0.001664  0.103319   \n",
      "14     0.285693  0.127764 -0.365324    ...     0.137205 -0.009927  0.159071   \n",
      "15     0.264832  0.129591 -0.366905    ...     0.098436 -0.019926 -0.001309   \n",
      "16     0.367824  0.119783 -0.407532    ...     0.111354 -0.023082  0.160573   \n",
      "17     0.259494  0.185185 -0.361639    ...     0.065524 -0.013946  0.166405   \n",
      "18     0.197030  0.176720 -0.402051    ...     0.083203 -0.010517  0.140349   \n",
      "19     0.345719  0.141291 -0.340079    ...     0.137557 -0.016372  0.160160   \n",
      "20     0.172252  0.167280 -0.481209    ...     0.121422 -0.028451  0.109289   \n",
      "21     0.276378  0.180244 -0.314023    ...     0.123142 -0.016370  0.149385   \n",
      "22     0.157362  0.315546 -0.393191    ...     0.145586 -0.003677  0.190528   \n",
      "23     0.373720  0.179608 -0.306049    ...     0.224222 -0.006104  0.096121   \n",
      "24     0.325717  0.297871 -0.398989    ...     0.206409  0.015456  0.214602   \n",
      "25     0.264334  0.269589 -0.428428    ...     0.152294 -0.002339  0.173945   \n",
      "26     0.289775  0.301878 -0.507993    ...     0.158412 -0.073247  0.119530   \n",
      "27     0.285636  0.250421 -0.438021    ...     0.135970 -0.031341  0.082042   \n",
      "28     0.317975  0.223776 -0.359402    ...     0.092093 -0.033573  0.128310   \n",
      "29     0.134313  0.306728 -0.589696    ...     0.170220 -0.034644  0.065737   \n",
      "...         ...       ...       ...    ...          ...       ...       ...   \n",
      "57970  0.130353  0.326585 -0.433676    ...     0.111359  0.017941  0.055627   \n",
      "57971  0.236846  0.329222 -0.273533    ...     0.164866  0.018595  0.114686   \n",
      "57972  0.231387  0.438012 -0.314343    ...     0.092386  0.018957  0.131161   \n",
      "57973  0.268974  0.423564 -0.422598    ...     0.099318  0.018955  0.126397   \n",
      "57974  0.186256  0.412125 -0.456706    ...     0.060330  0.005092  0.060888   \n",
      "57975  0.175963  0.354233 -0.499264    ...     0.046727 -0.032365 -0.023186   \n",
      "57976  0.182336  0.385911 -0.469915    ...     0.046155 -0.033421 -0.041952   \n",
      "57977  0.177559  0.387667 -0.516133    ...     0.050482 -0.034974 -0.037324   \n",
      "57978  0.135227  0.454171 -0.473347    ...     0.078939 -0.029057 -0.038856   \n",
      "57979  0.143923  0.453275 -0.441383    ...     0.083767 -0.014357 -0.025989   \n",
      "57980  0.146600  0.452657 -0.376712    ...     0.137638 -0.004283  0.024248   \n",
      "57981  0.188594  0.404414 -0.298209    ...     0.103008  0.002967 -0.024656   \n",
      "57982  0.229455  0.389649 -0.291452    ...     0.151297  0.016740 -0.013730   \n",
      "57983  0.218515  0.442219 -0.318944    ...     0.118370  0.023664  0.037516   \n",
      "57984  0.261241  0.483812 -0.422347    ...     0.124818  0.037098  0.116305   \n",
      "57985  0.161930  0.464444 -0.462250    ...     0.070194  0.017814  0.027868   \n",
      "57986  0.153058  0.361304 -0.491807    ...     0.085894  0.004124  0.006929   \n",
      "57987  0.265746  0.411877 -0.359567    ...     0.071311  0.014645  0.101180   \n",
      "57988  0.119424  0.415845 -0.470636    ...     0.094527  0.000546  0.000966   \n",
      "57989  0.197620  0.360500 -0.345982    ...     0.151413  0.001549 -0.039258   \n",
      "57990  0.228718  0.390348 -0.368936    ...     0.132916  0.006239  0.010454   \n",
      "57991  0.229112  0.314191 -0.412562    ...     0.129271  0.004761 -0.038102   \n",
      "57992  0.195579  0.414871 -0.541014    ...     0.119009 -0.017721 -0.046417   \n",
      "57993  0.237743  0.350580 -0.544225    ...     0.095293 -0.029082 -0.028555   \n",
      "57994  0.172879  0.407323 -0.542310    ...     0.067987 -0.053905 -0.034152   \n",
      "57995  0.137458  0.373028 -0.633378    ...     0.109930 -0.053731 -0.064926   \n",
      "57996  0.258700  0.367949 -0.326703    ...     0.075904 -0.011010 -0.017376   \n",
      "57997  0.078366  0.361633 -0.675724    ...     0.197443 -0.030415 -0.010921   \n",
      "57998  0.274565  0.380754 -0.278886    ...     0.216863  0.016763  0.058006   \n",
      "57999  0.157225  0.479757 -0.494623    ...     0.133687  0.019051  0.096333   \n",
      "\n",
      "             57        58        59        60        61        62        63  \n",
      "0      0.487939  0.511597  0.349602 -0.145424 -0.455004  0.441083  0.015762  \n",
      "1     -0.076822 -0.164030  0.893134  0.008492  0.016379  0.301379  0.005142  \n",
      "2     -0.032667 -0.108770  0.888038 -0.030944 -0.022160  0.497383 -0.016388  \n",
      "3     -0.030964 -0.047055  0.861368 -0.051400 -0.038630  0.666427 -0.020759  \n",
      "4     -0.036768 -0.044220  0.816845 -0.046780 -0.055321  0.660834 -0.026113  \n",
      "5     -0.064195 -0.058449  0.819727 -0.053925 -0.038737  0.674398 -0.017432  \n",
      "6     -0.048263 -0.037435  0.778645 -0.060062 -0.016980  0.652568 -0.019643  \n",
      "7     -0.030426 -0.038942  0.705063 -0.075229 -0.065626  0.657501 -0.067168  \n",
      "8      0.035158 -0.037259  0.774125 -0.105098 -0.029515  0.639903 -0.045360  \n",
      "9     -0.037739 -0.030386  0.660063 -0.127395 -0.085441  0.572246 -0.085825  \n",
      "10    -0.103659 -0.002936  0.692104 -0.162095 -0.067467  0.600933 -0.004912  \n",
      "11    -0.133821  0.000229  0.687867 -0.150174 -0.082718  0.557086 -0.009580  \n",
      "12    -0.200778 -0.011217  0.575031 -0.178972 -0.070315  0.479866 -0.006530  \n",
      "13    -0.151392  0.019952  0.523976 -0.228020 -0.025012  0.502675  0.033962  \n",
      "14    -0.185315  0.005921  0.560682 -0.257437 -0.066823  0.499618  0.007613  \n",
      "15    -0.078992 -0.023821  0.601956 -0.188844 -0.052808  0.455927 -0.025289  \n",
      "16    -0.142497  0.000680  0.599247 -0.224612 -0.105688  0.476499 -0.039322  \n",
      "17    -0.130926 -0.018574  0.462415 -0.183796 -0.049165  0.495711 -0.056372  \n",
      "18    -0.164547 -0.006038  0.537005 -0.174843 -0.052648  0.508777  0.009857  \n",
      "19    -0.267873 -0.016887  0.544652 -0.217414 -0.061035  0.447292  0.025399  \n",
      "20    -0.165588 -0.018657  0.525783 -0.214575 -0.087700  0.454747  0.008836  \n",
      "21    -0.232572 -0.008371  0.402627 -0.172029 -0.072603  0.491755 -0.008352  \n",
      "22    -0.105586 -0.042746  0.269446 -0.100759 -0.051021  0.420040 -0.000352  \n",
      "23    -0.236962 -0.018610  0.305138 -0.166258 -0.052423  0.425097  0.001632  \n",
      "24    -0.101726  0.002017  0.339098 -0.076670 -0.051397  0.542834 -0.036802  \n",
      "25    -0.183757 -0.035624  0.219369 -0.088381 -0.036086  0.476750 -0.031323  \n",
      "26    -0.001479 -0.051571  0.331985 -0.106514 -0.014167  0.477594  0.003510  \n",
      "27    -0.071645 -0.125622  0.219496 -0.098311 -0.047108  0.403606 -0.023932  \n",
      "28    -0.148694 -0.064050  0.278964 -0.128571 -0.075679  0.511326 -0.018728  \n",
      "29    -0.059973 -0.064885  0.340290 -0.142496 -0.040736  0.480546  0.002427  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "57970 -0.045913 -0.008770  0.240208 -0.202128 -0.064000  0.506817 -0.016151  \n",
      "57971 -0.048873 -0.027504  0.296122 -0.219357 -0.011090  0.495363  0.006836  \n",
      "57972  0.003050 -0.021194  0.255754 -0.220645  0.015945  0.475633 -0.028318  \n",
      "57973  0.008979 -0.005849  0.212789 -0.226085  0.000478  0.533338 -0.048143  \n",
      "57974  0.012008 -0.044465  0.209985 -0.209171  0.018234  0.518144 -0.012366  \n",
      "57975 -0.027510 -0.037411  0.281115 -0.240600 -0.064327  0.567132 -0.024934  \n",
      "57976 -0.008572 -0.028296  0.290684 -0.241742 -0.063863  0.553651 -0.022141  \n",
      "57977  0.040577 -0.020276  0.394226 -0.248845 -0.091150  0.619189  0.001419  \n",
      "57978  0.120106 -0.039908  0.308790 -0.213766 -0.039611  0.621562 -0.016499  \n",
      "57979  0.120085 -0.034538  0.279002 -0.178866 -0.037418  0.686107 -0.019626  \n",
      "57980  0.077643 -0.030678  0.234870 -0.195671 -0.045131  0.647741 -0.018247  \n",
      "57981 -0.019489  0.003301  0.189982 -0.192031 -0.065949  0.637561 -0.054356  \n",
      "57982  0.044297 -0.015794  0.141073 -0.180562 -0.052526  0.506708 -0.071454  \n",
      "57983  0.014762 -0.015850  0.158432 -0.188485 -0.023761  0.488421 -0.078969  \n",
      "57984  0.010804  0.012435  0.127149 -0.211506 -0.007523  0.552405 -0.114358  \n",
      "57985 -0.020193 -0.020347  0.132008 -0.177753 -0.040898  0.585626 -0.128857  \n",
      "57986 -0.013433 -0.030130  0.144585 -0.157042 -0.083954  0.529899 -0.038081  \n",
      "57987  0.022150  0.016064  0.201288 -0.175120 -0.034741  0.571221 -0.021937  \n",
      "57988 -0.004071 -0.002142  0.219341 -0.204778 -0.044670  0.609946 -0.002244  \n",
      "57989  0.070705 -0.053823  0.212121 -0.169185 -0.051292  0.595779 -0.014814  \n",
      "57990  0.060224 -0.045072  0.212901 -0.156397 -0.019823  0.590448 -0.010444  \n",
      "57991  0.013544 -0.030466  0.147855 -0.177137 -0.058685  0.529911 -0.032003  \n",
      "57992  0.090274 -0.036623  0.226309 -0.168383 -0.027933  0.585143 -0.041531  \n",
      "57993  0.043449 -0.014283  0.269813 -0.207322 -0.107440  0.622659 -0.042651  \n",
      "57994  0.048958 -0.017266  0.298736 -0.228503 -0.101989  0.622904 -0.068581  \n",
      "57995  0.097886 -0.028917  0.357461 -0.206370 -0.101968  0.598504 -0.042871  \n",
      "57996 -0.011964 -0.007390  0.330477 -0.190821 -0.114571  0.647670 -0.075800  \n",
      "57997  0.092337 -0.014345  0.358546 -0.229902 -0.087724  0.596243 -0.040733  \n",
      "57998 -0.046772  0.015154  0.278924 -0.179216 -0.059714  0.656494 -0.071244  \n",
      "57999 -0.017796 -0.006878  0.135807 -0.199476 -0.033197  0.622646 -0.091065  \n",
      "\n",
      "[58000 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "average_hidden_pd = pd.DataFrame(np.row_stack(average_hidden_list))\n",
    "final_hidden_pd = pd.DataFrame(np.row_stack(final_hidden_list))\n",
    "hidden_state = pd.DataFrame(np.row_stack(hs_list))\n",
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_hidden_pd.to_csv(\"emi_neg_reps_5A_4.csv\")\n",
    "final_hidden_pd.to_csv(\"emi_neg_finalhidden_5A_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_loc = \"C:\\\\Users\\\\pkinn\\\\Documents\\\\UniRep\\\\full representations\\\\emi larger set\\\\\"\n",
    "data_name = 'emi_neg_reps_5A_4'\n",
    "file_append = '.pickle'\n",
    "\n",
    "\n",
    "fn = save_loc + data_name + 'avg_hidden' + file_append\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(average_hidden_list, f)\n",
    "\n",
    "fn = save_loc + data_name + 'final_hidden' + file_append\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(final_hidden_list, f)\n",
    "\n",
    "fn = save_loc + data_name + 'final_cell' + file_append\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(final_cell_list, f)\n",
    "   \n",
    "fn = save_loc + data_name + 'hidden_state' + file_append\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(hs_list, f)\n",
    "   \n",
    "fn = save_loc + data_name + 'all_output_hs' + file_append\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump([average_hidden_list, final_hidden_list, final_cell_list, hs_list], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
